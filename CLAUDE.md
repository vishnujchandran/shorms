- this file is symlinked to GEMINI.md
- make this file very short, its loaded on each request
- conserve tokens, I have very few
- LLM agent is coding inside a restricted environment/container
- so see "~/.info.json" to see properties of the container
- use one of the externally mapped port to start the server
